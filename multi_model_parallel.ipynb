{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Multi Model Parallel Inference\n",
    "\n",
    "OpenVINO provides [Asynchronous Inference Request](https://docs.openvino.ai/2023.2/openvino_docs_ov_plugin_dg_async_infer_request.html) for parallel inference.\n",
    "\n",
    "Suppose that we are processing a video stream, of which each frame needs to call inference of three different models once."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b2312be43feee4ec"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Define Benchmark Function"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0e844bf81a76905"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "from openvino import Core\n",
    "from openvino import properties\n",
    "from openvino.properties.hint import PerformanceMode, SchedulingCoreType\n",
    "\n",
    "\n",
    "def benchmark_model(infer_one_frame):\n",
    "    sec = 10\n",
    "    count = 0\n",
    "    start = time()\n",
    "    while time() - start < sec:\n",
    "        infer_one_frame()\n",
    "        count += 1\n",
    "\n",
    "    print(f\"FPS={count / sec:.2f}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b3359e042b294834",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Single Model Parallel Inference\n",
    "\n",
    "Load OpenVINO models path"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f77f2f455d404d4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "res18 = \"models/resnet18/int8/model.xml\"\n",
    "res50 = \"models/resnet50/int8/model.xml\"\n",
    "res101 = \"models/resnet101/int8/model.xml\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "330b32b51b098fd4",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define infer_one_frame()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f423a374762c1578"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def single_res18_sync_infer():\n",
    "    core = Core()\n",
    "    compiled_res18 = core.compile_model(res18, \"CPU\")\n",
    "    inputs = np.random.randn(1, 3, 224, 224)\n",
    "    req = compiled_res18.create_infer_request()\n",
    "\n",
    "    def infer_one_frame():\n",
    "        req.infer(inputs)\n",
    "        req.infer(inputs)\n",
    "        req.infer(inputs)\n",
    "\n",
    "    return infer_one_frame\n",
    "\n",
    "\n",
    "def single_res18_async_infer():\n",
    "    req_count = 3\n",
    "\n",
    "    # Reason use PCORE_ONLY here:\n",
    "    # The inference time on E-core is longer than three times of P-core. (Test by Core-13700K)\n",
    "    # So, when we schedule three inferences, three P-core inferences is faster than two P-core calls and one E-core inference.\n",
    "    config = {\n",
    "        properties.hint.performance_mode(): PerformanceMode.THROUGHPUT,\n",
    "        properties.hint.num_requests(): req_count,\n",
    "        properties.hint.scheduling_core_type(): SchedulingCoreType.PCORE_ONLY,\n",
    "    }\n",
    "\n",
    "    core = Core()\n",
    "    compiled_res18 = core.compile_model(res18, \"CPU\", config)\n",
    "    infer_reqs = [compiled_res18.create_infer_request() for _ in range(req_count)]\n",
    "    inputs = np.random.randn(1, 3, 224, 224)\n",
    "\n",
    "    def infer_one_frame():\n",
    "        for req in infer_reqs:\n",
    "            req.start_async(inputs)\n",
    "\n",
    "        for req in infer_reqs:\n",
    "            req.wait()\n",
    "\n",
    "    return infer_one_frame"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d19bc1200a303bef",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Benchmark sync inferences"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "263c15f947b58696"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "benchmark_model(single_res18_sync_infer())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e07faff7f3c6939e",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Benchmark async inference"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a5db804a273acd39"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "benchmark_model(single_res18_async_infer())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f30e60b784c982b4",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Three Models Parallel Inference: all resnet18.\n",
    "\n",
    "Define infer_one_frame() "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d46e021330e60964"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def three_res18_sync_infer():\n",
    "    core = Core()\n",
    "\n",
    "    model_count = 3\n",
    "    compiled_res18s = [core.compile_model(res18, \"CPU\") for _ in range(model_count)]\n",
    "    reqs = [m.create_infer_request() for m in compiled_res18s]\n",
    "\n",
    "    inputs = np.random.randn(1, 3, 224, 224)\n",
    "\n",
    "    def infer_one_frame():\n",
    "        for req in reqs:\n",
    "            req.infer(inputs)\n",
    "\n",
    "    return infer_one_frame\n",
    "\n",
    "\n",
    "def three_res18_async_infer():\n",
    "    p_core_config = {\n",
    "        properties.hint.scheduling_core_type(): SchedulingCoreType.PCORE_ONLY,\n",
    "    }\n",
    "\n",
    "    core = Core()\n",
    "    compiled_res18s = [\n",
    "        core.compile_model(res18, \"CPU\", p_core_config),\n",
    "        core.compile_model(res18, \"CPU\", p_core_config),\n",
    "        core.compile_model(res18, \"CPU\", p_core_config),\n",
    "    ]\n",
    "\n",
    "    reqs = [m.create_infer_request() for m in compiled_res18s]\n",
    "    inputs = np.random.randn(1, 3, 224, 224)\n",
    "\n",
    "    def infer_one_frame():\n",
    "        for req in reqs:\n",
    "            req.start_async(inputs)\n",
    "\n",
    "        for req in reqs:\n",
    "            req.wait()\n",
    "\n",
    "    return infer_one_frame"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4a5df8c65ca393fc",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Benchmark sync inferences"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4856f5f966be4c0a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "benchmark_model(three_res18_sync_infer())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e2cd2b104ba4231",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Benchmark async inference"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "35588e535457c333"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "benchmark_model(three_res18_async_infer())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "93e3013f48d75a3d",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Three Models Parallel Inference: resnet18, resnet50, and resnet101\n",
    "\n",
    "Define infer_one_frame() "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "324c224378f1f503"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def three_sync_infer():\n",
    "    core = Core()\n",
    "\n",
    "    compiled_models = [\n",
    "        core.compile_model(res18, \"CPU\"),\n",
    "        core.compile_model(res50, \"CPU\"),\n",
    "        core.compile_model(res101, \"CPU\"),\n",
    "    ]\n",
    "\n",
    "    reqs = [m.create_infer_request() for m in compiled_models]\n",
    "    inputs = np.random.randn(1, 3, 224, 224)\n",
    "\n",
    "    def infer_one_frame():\n",
    "        for req in reqs:\n",
    "            req.infer(inputs)\n",
    "\n",
    "    return infer_one_frame\n",
    "\n",
    "\n",
    "def three_async_infer():\n",
    "    p_core_config = {\n",
    "        properties.hint.scheduling_core_type(): SchedulingCoreType.PCORE_ONLY,\n",
    "    }\n",
    "\n",
    "    e_core_config = {\n",
    "        properties.hint.scheduling_core_type(): SchedulingCoreType.ECORE_ONLY,\n",
    "    }\n",
    "\n",
    "    core = Core()\n",
    "    compiled_res18s = [\n",
    "        core.compile_model(res18, \"CPU\", e_core_config),\n",
    "        core.compile_model(res50, \"CPU\", p_core_config),\n",
    "        core.compile_model(res101, \"CPU\", p_core_config),\n",
    "    ]\n",
    "\n",
    "    reqs = [m.create_infer_request() for m in compiled_res18s]\n",
    "    inputs = np.random.randn(1, 3, 224, 224)\n",
    "\n",
    "    def infer_one_frame():\n",
    "        for req in reqs:\n",
    "            req.start_async(inputs)\n",
    "\n",
    "        for req in reqs:\n",
    "            req.wait()\n",
    "\n",
    "    return infer_one_frame"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7482e877def41e8a",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Benchmark sync inferences"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "12a094f3947f553a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "benchmark_model(three_sync_infer())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "40b8c861c28b9f20",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Benchmark async inferences"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e6512e9eabbd04e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "benchmark_model(three_async_infer())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cf85f9c7e759d9e7",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
